# 0x11. Attention

In the following directory we will find a copilation of files wich porpuse is to understand the concepts of using   Attention and help us to answer the next questions.
-   What is the attention mechanism?
-   How to apply attention to RNNs
-   What is a transformer?
-   How to create an encoder-decoder transformer model
-   What is GPT?
-   What is BERT?
-   What is self-supervised learning?
-   How to use BERT for specific NLP tasks
-   What is SQuAD? GLUE?

## Files
 - 0-rnn_encoder.py
 - 1-self_attention.py
 - 2-rnn_decoder.py
 - 4-positional_encoding.py
 - 5-sdp_attention.py
 - 6-multihead_attention.py
 - 7-transformer_encoder_block.py
 - 8-transformer_decoder_block.py
 - 9-transformer_encoder.py
 - 10-transformer_decoder.py
 - 11-transformer.py