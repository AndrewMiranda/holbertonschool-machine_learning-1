# 0x03. Optimization

In the following directory we will find a copilation of files wich porpuse is to understand the concept of optimization in a neural network and hel us to answe the enxt questions.
 -   What is a hyperparameter?
 -   How and why do you normalize your input data?
 -   What is a saddle point?
 -   What is stochastic gradient descent?
 -   What is mini-batch gradient descent?
 -   What is a moving average? How do you implement it?
 -   What is gradient descent with momentum? How do you implement it?
 -   What is RMSProp? How do you implement it?
 -   What is Adam optimization? How do you implement it?
 -   What is learning rate decay? How do you implement it?
 -   What is batch normalization? How do you implement it?
## Files
 - 0-norm_constants.py
 - 1-normalize.py
 - 2-shuffle_data.py
 - 3-mini_batch.py
 - 4-moving_average.py
 - 5-momentum.py
 - 6-momentum.py
 - 7-RMSProp.py
 - 8-RMSProp.py
 - 9-Adam.py
 - 10-Adam.py
 - 11-learning_rate_decay.py
 - 12-learning_rate_decay.py
 - 13-batch_norm.py
 - 14-batch_norm.py
 - 15-model.py
 - 
