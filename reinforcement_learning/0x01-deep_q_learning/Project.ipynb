{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-01 12:59:17.871265: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-01 12:59:17.871341: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import argparse\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Convolution2D, Permute\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Processor\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = (84, 84)\n",
    "WINDOW_LENGTH = 4\n",
    "\n",
    "class AtariProcessor(Processor):\n",
    "    \"\"\"Class type Atari preprocessor based in keras-rl \"\"\"\n",
    "    def process_observation(self, observation):\n",
    "        # (height, width, channel)\n",
    "        assert observation.ndim == 3\n",
    "        # resize image\n",
    "        img = Image.fromarray(observation)\n",
    "        img = img.resize(INPUT_SHAPE).convert('L')\n",
    "        processed_observation = np.array(img)\n",
    "        assert processed_observation.shape == INPUT_SHAPE\n",
    "        return processed_observation.astype('uint8')\n",
    "\n",
    "    def process_state_batch(self, batch):\n",
    "        processed_batch = batch.astype('float32') / 255.\n",
    "        return processed_batch\n",
    "\n",
    "    def process_reward(self, reward):\n",
    "        return np.clip(reward, -1., 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_action):\n",
    "    input_shape = (WINDOW_LENGTH,) + INPUT_SHAPE\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Permute((2, 3, 1), input_shape=input_shape))\n",
    "    model.add(Convolution2D(32, (8, 8), strides=(4, 4)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(64, (4, 4), strides=(2, 2)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(64, (3, 3), strides=(1, 1)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(num_action))\n",
    "    model.add(Activation('linear'))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " permute (Permute)           (None, 84, 84, 4)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 20, 20, 32)        8224      \n",
      "                                                                 \n",
      " activation (Activation)     (None, 20, 20, 32)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 9, 9, 64)          32832     \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 9, 9, 64)          0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 7, 7, 64)          36928     \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 7, 7, 64)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3136)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               1606144   \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4)                 2052      \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 4)                 0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 4)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,686,180\n",
      "Trainable params: 1,686,180\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juand0145/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "2022-07-01 15:08:52.889482: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/juand0145/anaconda3/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2022-07-01 15:08:52.908551: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-07-01 15:08:52.933150: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-HTDB37B): /proc/driver/nvidia/version does not exist\n",
      "2022-07-01 15:08:53.026848: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-01 15:08:53.214277: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 17500 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juand0145/anaconda3/lib/python3.9/site-packages/keras/engine/training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   224/17500: episode: 1, duration: 3.786s, episode steps: 224, steps per second:  59, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.594 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   403/17500: episode: 2, duration: 0.923s, episode steps: 179, steps per second: 194, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.436 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   629/17500: episode: 3, duration: 1.121s, episode steps: 226, steps per second: 202, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.451 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   972/17500: episode: 4, duration: 1.735s, episode steps: 343, steps per second: 198, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.440 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  1138/17500: episode: 5, duration: 0.950s, episode steps: 166, steps per second: 175, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.518 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  1373/17500: episode: 6, duration: 1.128s, episode steps: 235, steps per second: 208, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.549 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  1710/17500: episode: 7, duration: 1.666s, episode steps: 337, steps per second: 202, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.513 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  1953/17500: episode: 8, duration: 1.173s, episode steps: 243, steps per second: 207, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.650 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  2332/17500: episode: 9, duration: 2.180s, episode steps: 379, steps per second: 174, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.414 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  2644/17500: episode: 10, duration: 1.516s, episode steps: 312, steps per second: 206, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.545 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  3015/17500: episode: 11, duration: 1.726s, episode steps: 371, steps per second: 215, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.485 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  3264/17500: episode: 12, duration: 1.171s, episode steps: 249, steps per second: 213, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.610 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  3552/17500: episode: 13, duration: 1.354s, episode steps: 288, steps per second: 213, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.517 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  3827/17500: episode: 14, duration: 1.319s, episode steps: 275, steps per second: 209, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.462 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  4113/17500: episode: 15, duration: 1.375s, episode steps: 286, steps per second: 208, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.483 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  4379/17500: episode: 16, duration: 1.259s, episode steps: 266, steps per second: 211, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.432 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  4604/17500: episode: 17, duration: 1.259s, episode steps: 225, steps per second: 179, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.622 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  4928/17500: episode: 18, duration: 1.680s, episode steps: 324, steps per second: 193, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.404 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  5206/17500: episode: 19, duration: 1.351s, episode steps: 278, steps per second: 206, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.482 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  5389/17500: episode: 20, duration: 0.899s, episode steps: 183, steps per second: 204, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.508 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  5667/17500: episode: 21, duration: 1.372s, episode steps: 278, steps per second: 203, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.471 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  5839/17500: episode: 22, duration: 0.852s, episode steps: 172, steps per second: 202, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.477 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  6179/17500: episode: 23, duration: 1.660s, episode steps: 340, steps per second: 205, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.479 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  6353/17500: episode: 24, duration: 0.882s, episode steps: 174, steps per second: 197, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.575 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  6601/17500: episode: 25, duration: 2.013s, episode steps: 248, steps per second: 123, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.512 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  6891/17500: episode: 26, duration: 1.505s, episode steps: 290, steps per second: 193, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.517 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  7102/17500: episode: 27, duration: 1.695s, episode steps: 211, steps per second: 124, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.403 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  7402/17500: episode: 28, duration: 1.433s, episode steps: 300, steps per second: 209, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.493 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  7579/17500: episode: 29, duration: 1.623s, episode steps: 177, steps per second: 109, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.531 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  7944/17500: episode: 30, duration: 1.762s, episode steps: 365, steps per second: 207, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.501 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  8295/17500: episode: 31, duration: 1.649s, episode steps: 351, steps per second: 213, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.430 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  8480/17500: episode: 32, duration: 0.865s, episode steps: 185, steps per second: 214, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.486 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  8662/17500: episode: 33, duration: 0.848s, episode steps: 182, steps per second: 215, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  9111/17500: episode: 34, duration: 2.112s, episode steps: 449, steps per second: 213, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.499 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  9464/17500: episode: 35, duration: 1.656s, episode steps: 353, steps per second: 213, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.493 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  9793/17500: episode: 36, duration: 1.731s, episode steps: 329, steps per second: 190, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.587 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      " 10066/17500: episode: 37, duration: 1.398s, episode steps: 273, steps per second: 195, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.524 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      " 10407/17500: episode: 38, duration: 1.600s, episode steps: 341, steps per second: 213, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.413 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      " 10744/17500: episode: 39, duration: 1.542s, episode steps: 337, steps per second: 219, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.421 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      " 11044/17500: episode: 40, duration: 1.408s, episode steps: 300, steps per second: 213, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.503 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      " 11254/17500: episode: 41, duration: 0.979s, episode steps: 210, steps per second: 215, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.367 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      " 11481/17500: episode: 42, duration: 1.076s, episode steps: 227, steps per second: 211, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.476 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      " 11827/17500: episode: 43, duration: 1.627s, episode steps: 346, steps per second: 213, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.442 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      " 12083/17500: episode: 44, duration: 1.167s, episode steps: 256, steps per second: 219, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.352 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      " 12251/17500: episode: 45, duration: 0.777s, episode steps: 168, steps per second: 216, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.387 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      " 12496/17500: episode: 46, duration: 1.150s, episode steps: 245, steps per second: 213, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.486 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      " 12774/17500: episode: 47, duration: 1.333s, episode steps: 278, steps per second: 209, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.453 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      " 12948/17500: episode: 48, duration: 0.829s, episode steps: 174, steps per second: 210, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.506 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      " 13186/17500: episode: 49, duration: 1.107s, episode steps: 238, steps per second: 215, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.471 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      " 13368/17500: episode: 50, duration: 0.859s, episode steps: 182, steps per second: 212, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.566 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      " 13558/17500: episode: 51, duration: 0.901s, episode steps: 190, steps per second: 211, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.547 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      " 13734/17500: episode: 52, duration: 0.866s, episode steps: 176, steps per second: 203, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.602 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      " 13898/17500: episode: 53, duration: 0.807s, episode steps: 164, steps per second: 203, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.445 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      " 14119/17500: episode: 54, duration: 1.048s, episode steps: 221, steps per second: 211, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.543 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      " 14387/17500: episode: 55, duration: 1.276s, episode steps: 268, steps per second: 210, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.534 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      " 14619/17500: episode: 56, duration: 1.113s, episode steps: 232, steps per second: 209, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.418 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      " 14814/17500: episode: 57, duration: 0.912s, episode steps: 195, steps per second: 214, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.579 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      " 15091/17500: episode: 58, duration: 1.438s, episode steps: 277, steps per second: 193, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.451 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      " 15364/17500: episode: 59, duration: 1.498s, episode steps: 273, steps per second: 182, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.498 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      " 15810/17500: episode: 60, duration: 2.189s, episode steps: 446, steps per second: 204, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.489 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      " 16111/17500: episode: 61, duration: 1.552s, episode steps: 301, steps per second: 194, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.505 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      " 16423/17500: episode: 62, duration: 1.538s, episode steps: 312, steps per second: 203, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.471 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      " 16725/17500: episode: 63, duration: 1.637s, episode steps: 302, steps per second: 184, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.417 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      " 16901/17500: episode: 64, duration: 0.857s, episode steps: 176, steps per second: 205, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.602 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      " 17175/17500: episode: 65, duration: 1.301s, episode steps: 274, steps per second: 211, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.453 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "done, took 90.762 seconds\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Breakout-v0\")\n",
    "env.reset()\n",
    "num_action = env.action_space.n\n",
    "window = 4\n",
    "model = build_model(num_action)\n",
    "model.summary()\n",
    "memory = SequentialMemory(limit=1000000, window_length=WINDOW_LENGTH)\n",
    "processor = AtariProcessor()\n",
    "\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1.,\n",
    "                              value_min=.1, value_test=.05, nb_steps=1000000)\n",
    "\n",
    "dqn = DQNAgent(model=model, nb_actions=num_action, policy=policy,\n",
    "               memory=memory, processor=processor, nb_steps_warmup=50000,\n",
    "               gamma=.99, target_model_update=10000, train_interval=4,\n",
    "               delta_clip=1.)\n",
    "\n",
    "dqn.compile(Adam(lr=.00025), metrics=['mae'])\n",
    "# training\n",
    "dqn.fit(env,\n",
    "        nb_steps=17500,\n",
    "        log_interval=10000,\n",
    "        visualize=False,\n",
    "        verbose=2)\n",
    "\n",
    "# save the final weights.\n",
    "dqn.save_weights('policy.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juand0145/anaconda3/lib/python3.9/site-packages/keras/engine/training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: reward: 0.000, steps: 10000\n",
      "Episode 2: reward: 0.000, steps: 10000\n",
      "Episode 3: reward: 0.000, steps: 10000\n",
      "Episode 4: reward: 0.000, steps: 10000\n",
      "Episode 5: reward: 0.000, steps: 10000\n",
      "Episode 6: reward: 0.000, steps: 10000\n",
      "Episode 7: reward: 0.000, steps: 10000\n",
      "Episode 8: reward: 0.000, steps: 10000\n",
      "Episode 9: reward: 0.000, steps: 10000\n",
      "Episode 10: reward: 0.000, steps: 10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efe22b22280>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"Breakout-v0\")\n",
    "env.reset()\n",
    "num_actions = env.action_space.n\n",
    "model = build_model(num_action)  # deep conv net\n",
    "memory = SequentialMemory(limit=1000000, window_length=WINDOW_LENGTH)\n",
    "processor = AtariProcessor()\n",
    "dqn = DQNAgent(model=model, nb_actions=num_actions,\n",
    "            processor=processor, memory=memory)\n",
    "dqn.compile(Adam(learning_rate=.00025), metrics=['mae'])\n",
    "\n",
    "# load weights.\n",
    "dqn.load_weights('policy.h5')\n",
    "\n",
    "# evaluate algorithm for 10 episodes.\n",
    "dqn.test(env, nb_episodes=10, visualize=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "518b19cc3605565ad20a20daef124b2c7c25f4f3f8506b9cb812e28a2f17715b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
